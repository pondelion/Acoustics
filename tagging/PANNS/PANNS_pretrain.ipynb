{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PANNS_pretrain.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPXsh6G/HS/fPEJ2VYtFgMG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"hufIDDHwpyAk","colab_type":"code","outputId":"a53501f3-ae69-4c26-e8a6-d63400369fbb","executionInfo":{"status":"ok","timestamp":1586246751769,"user_tz":-540,"elapsed":6323,"user":{"displayName":"yoshihiro matsumoto","photoUrl":"","userId":"08203775409780696699"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["!git clone https://github.com/qiuqiangkong/audioset_tagging_cnn.git"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'audioset_tagging_cnn'...\n","remote: Enumerating objects: 266, done.\u001b[K\n","Receiving objects:   0% (1/266)   \rReceiving objects:   1% (3/266)   \rReceiving objects:   2% (6/266)   \rReceiving objects:   3% (8/266)   \rReceiving objects:   4% (11/266)   \rReceiving objects:   5% (14/266)   \rReceiving objects:   6% (16/266)   \rReceiving objects:   7% (19/266)   \rReceiving objects:   8% (22/266)   \rReceiving objects:   9% (24/266)   \rReceiving objects:  10% (27/266)   \rReceiving objects:  11% (30/266)   \rReceiving objects:  12% (32/266)   \rReceiving objects:  13% (35/266)   \rReceiving objects:  14% (38/266)   \rReceiving objects:  15% (40/266)   \rReceiving objects:  16% (43/266)   \rReceiving objects:  17% (46/266)   \rReceiving objects:  18% (48/266)   \rReceiving objects:  19% (51/266)   \rReceiving objects:  20% (54/266)   \rReceiving objects:  21% (56/266)   \rReceiving objects:  22% (59/266)   \rReceiving objects:  23% (62/266)   \rReceiving objects:  24% (64/266)   \rReceiving objects:  25% (67/266)   \rReceiving objects:  26% (70/266)   \rReceiving objects:  27% (72/266)   \rReceiving objects:  28% (75/266)   \rReceiving objects:  29% (78/266)   \rReceiving objects:  30% (80/266)   \rReceiving objects:  31% (83/266)   \rReceiving objects:  32% (86/266)   \rReceiving objects:  33% (88/266)   \rReceiving objects:  34% (91/266)   \rReceiving objects:  35% (94/266)   \rReceiving objects:  36% (96/266)   \rReceiving objects:  37% (99/266)   \rReceiving objects:  38% (102/266)   \rReceiving objects:  39% (104/266)   \rReceiving objects:  40% (107/266)   \rReceiving objects:  41% (110/266)   \rReceiving objects:  42% (112/266)   \rReceiving objects:  43% (115/266)   \rReceiving objects:  44% (118/266)   \rReceiving objects:  45% (120/266)   \rReceiving objects:  46% (123/266)   \rReceiving objects:  47% (126/266)   \rReceiving objects:  48% (128/266)   \rReceiving objects:  49% (131/266)   \rReceiving objects:  50% (133/266)   \rReceiving objects:  51% (136/266)   \rReceiving objects:  52% (139/266)   \rReceiving objects:  53% (141/266)   \rReceiving objects:  54% (144/266)   \rReceiving objects:  55% (147/266)   \rReceiving objects:  56% (149/266)   \rReceiving objects:  57% (152/266)   \rReceiving objects:  58% (155/266)   \rReceiving objects:  59% (157/266)   \rReceiving objects:  60% (160/266)   \rReceiving objects:  61% (163/266)   \rReceiving objects:  62% (165/266)   \rReceiving objects:  63% (168/266)   \rReceiving objects:  64% (171/266)   \rReceiving objects:  65% (173/266)   \rReceiving objects:  66% (176/266)   \rReceiving objects:  67% (179/266)   \rReceiving objects:  68% (181/266)   \rReceiving objects:  69% (184/266)   \rReceiving objects:  70% (187/266)   \rReceiving objects:  71% (189/266)   \rReceiving objects:  72% (192/266)   \rReceiving objects:  73% (195/266)   \rReceiving objects:  74% (197/266)   \rReceiving objects:  75% (200/266)   \rReceiving objects:  76% (203/266)   \rremote: Total 266 (delta 0), reused 0 (delta 0), pack-reused 266\u001b[K\n","Receiving objects:  77% (205/266)   \rReceiving objects:  78% (208/266)   \rReceiving objects:  79% (211/266)   \rReceiving objects:  80% (213/266)   \rReceiving objects:  81% (216/266)   \rReceiving objects:  82% (219/266)   \rReceiving objects:  83% (221/266)   \rReceiving objects:  84% (224/266)   \rReceiving objects:  85% (227/266)   \rReceiving objects:  86% (229/266)   \rReceiving objects:  87% (232/266)   \rReceiving objects:  88% (235/266)   \rReceiving objects:  89% (237/266)   \rReceiving objects:  90% (240/266)   \rReceiving objects:  91% (243/266)   \rReceiving objects:  92% (245/266)   \rReceiving objects:  93% (248/266)   \rReceiving objects:  94% (251/266)   \rReceiving objects:  95% (253/266)   \rReceiving objects:  96% (256/266)   \rReceiving objects:  97% (259/266)   \rReceiving objects:  98% (261/266)   \rReceiving objects:  99% (264/266)   \rReceiving objects: 100% (266/266)   \rReceiving objects: 100% (266/266), 1.44 MiB | 8.58 MiB/s, done.\n","Resolving deltas:   0% (0/148)   \rResolving deltas:  12% (18/148)   \rResolving deltas:  13% (20/148)   \rResolving deltas:  16% (24/148)   \rResolving deltas:  30% (45/148)   \rResolving deltas:  32% (48/148)   \rResolving deltas:  34% (51/148)   \rResolving deltas:  43% (64/148)   \rResolving deltas:  45% (67/148)   \rResolving deltas:  50% (74/148)   \rResolving deltas:  52% (78/148)   \rResolving deltas:  55% (82/148)   \rResolving deltas:  57% (85/148)   \rResolving deltas:  60% (90/148)   \rResolving deltas:  63% (94/148)   \rResolving deltas:  64% (95/148)   \rResolving deltas:  66% (98/148)   \rResolving deltas:  72% (108/148)   \rResolving deltas:  73% (109/148)   \rResolving deltas:  77% (115/148)   \rResolving deltas:  91% (135/148)   \rResolving deltas:  92% (137/148)   \rResolving deltas:  93% (139/148)   \rResolving deltas:  96% (143/148)   \rResolving deltas:  97% (145/148)   \rResolving deltas:  98% (146/148)   \rResolving deltas:  99% (147/148)   \rResolving deltas: 100% (148/148)   \rResolving deltas: 100% (148/148), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9qYoYc5rp5bJ","colab_type":"code","outputId":"59dc5e78-7e1e-448f-a5ab-1e17aeb5e32c","executionInfo":{"status":"ok","timestamp":1586246753942,"user_tz":-540,"elapsed":8106,"user":{"displayName":"yoshihiro matsumoto","photoUrl":"","userId":"08203775409780696699"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34maudioset_tagging_cnn\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3CwVBHmYp-4T","colab_type":"code","outputId":"bcb60354-2bc0-4d9a-e559-e98f01b0ada0","executionInfo":{"status":"ok","timestamp":1586246753943,"user_tz":-540,"elapsed":7805,"user":{"displayName":"yoshihiro matsumoto","photoUrl":"","userId":"08203775409780696699"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd audioset_tagging_cnn"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/audioset_tagging_cnn\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JUjCcgpcqDrb","colab_type":"code","outputId":"612bfc51-9e9c-44a5-f077-053077b31c1e","executionInfo":{"status":"ok","timestamp":1586246863966,"user_tz":-540,"elapsed":117177,"user":{"displayName":"yoshihiro matsumoto","photoUrl":"","userId":"08203775409780696699"}},"colab":{"base_uri":"https://localhost:8080/","height":773}},"source":["!pip install -r requirements.txt"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting matplotlib==3.0.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/69/f5e05f578585ed9935247be3788b374f90701296a70c8871bcd6d21edb00/matplotlib-3.0.3-cp36-cp36m-manylinux1_x86_64.whl (13.0MB)\n","\u001b[K     |████████████████████████████████| 13.0MB 223kB/s \n","\u001b[?25hCollecting soundfile==0.10.3.post1\n","  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n","Requirement already satisfied: librosa==0.6.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.6.3)\n","Collecting torch==1.0.1.post2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/ca/dd2c64f8ab5e7985c4af6e62da933849293906edcdb70dac679c93477733/torch-1.0.1.post2-cp36-cp36m-manylinux1_x86_64.whl (582.5MB)\n","\u001b[K     |████████████████████████████████| 582.5MB 27kB/s \n","\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3->-r requirements.txt (line 1)) (2.4.6)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3->-r requirements.txt (line 1)) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3->-r requirements.txt (line 1)) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3->-r requirements.txt (line 1)) (1.2.0)\n","Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3->-r requirements.txt (line 1)) (1.18.2)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile==0.10.3.post1->-r requirements.txt (line 2)) (1.14.0)\n","Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa==0.6.3->-r requirements.txt (line 3)) (1.12.0)\n","Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa==0.6.3->-r requirements.txt (line 3)) (0.14.1)\n","Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.6.3->-r requirements.txt (line 3)) (2.1.8)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.6.3->-r requirements.txt (line 3)) (1.4.1)\n","Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.6.3->-r requirements.txt (line 3)) (0.47.0)\n","Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.6.3->-r requirements.txt (line 3)) (4.4.2)\n","Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.6.3->-r requirements.txt (line 3)) (0.2.2)\n","Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.6.3->-r requirements.txt (line 3)) (0.22.2.post1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile==0.10.3.post1->-r requirements.txt (line 2)) (2.20)\n","Requirement already satisfied: llvmlite>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa==0.6.3->-r requirements.txt (line 3)) (0.31.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa==0.6.3->-r requirements.txt (line 3)) (46.1.3)\n","\u001b[31mERROR: torchvision 0.5.0 has requirement torch==1.4.0, but you'll have torch 1.0.1.post2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: matplotlib, soundfile, torch\n","  Found existing installation: matplotlib 3.2.1\n","    Uninstalling matplotlib-3.2.1:\n","      Successfully uninstalled matplotlib-3.2.1\n","  Found existing installation: torch 1.4.0\n","    Uninstalling torch-1.4.0:\n","      Successfully uninstalled torch-1.4.0\n","Successfully installed matplotlib-3.0.3 soundfile-0.10.3.post1 torch-1.0.1.post2\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["matplotlib","mpl_toolkits"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"wcPxKSN2qHHA","colab_type":"code","outputId":"9de11f22-84c9-43e1-b89c-83fe25376d8b","executionInfo":{"status":"ok","timestamp":1586246865940,"user_tz":-540,"elapsed":117633,"user":{"displayName":"yoshihiro matsumoto","photoUrl":"","userId":"08203775409780696699"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"source":["!wget https://github.com/qiuqiangkong/audioset_tagging_cnn/blob/master/examples/R9_ZSCveAHg_7s.wav"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-04-07 08:07:44--  https://github.com/qiuqiangkong/audioset_tagging_cnn/blob/master/examples/R9_ZSCveAHg_7s.wav\n","Resolving github.com (github.com)... 140.82.113.4\n","Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘R9_ZSCveAHg_7s.wav’\n","\n","\rR9_ZSCveAHg_7s.wav      [<=>                 ]       0  --.-KB/s               \rR9_ZSCveAHg_7s.wav      [ <=>                ]  64.69K  --.-KB/s    in 0.05s   \n","\n","2020-04-07 08:07:44 (1.25 MB/s) - ‘R9_ZSCveAHg_7s.wav’ saved [66239]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0V5lhpP9s11j","colab_type":"code","outputId":"1534b4c7-4207-4561-b25d-3fcd074f509f","executionInfo":{"status":"ok","timestamp":1586247215618,"user_tz":-540,"elapsed":16082,"user":{"displayName":"yoshihiro matsumoto","photoUrl":"","userId":"08203775409780696699"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"source":["!wget https://zenodo.org/record/3576403/files/Cnn10_mAP%3D0.380.pth?download=1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-04-07 08:13:20--  https://zenodo.org/record/3576403/files/Cnn10_mAP%3D0.380.pth?download=1\n","Resolving zenodo.org (zenodo.org)... 188.184.117.155\n","Connecting to zenodo.org (zenodo.org)|188.184.117.155|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 156800771 (150M) [application/octet-stream]\n","Saving to: ‘Cnn10_mAP=0.380.pth?download=1’\n","\n","Cnn10_mAP=0.380.pth 100%[===================>] 149.54M  11.1MB/s    in 13s     \n","\n","2020-04-07 08:13:33 (12.0 MB/s) - ‘Cnn10_mAP=0.380.pth?download=1’ saved [156800771/156800771]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-L20woMDtGkc","colab_type":"code","outputId":"7cf632c8-1e41-43da-f7f1-b2970d7d06a2","executionInfo":{"status":"ok","timestamp":1586247365319,"user_tz":-540,"elapsed":97214,"user":{"displayName":"yoshihiro matsumoto","photoUrl":"","userId":"08203775409780696699"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"source":["!wget https://zenodo.org/record/3576403/files/Cnn14_mAP%3D0.431.pth?download=1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-04-07 08:14:28--  https://zenodo.org/record/3576403/files/Cnn14_mAP%3D0.431.pth?download=1\n","Resolving zenodo.org (zenodo.org)... 188.184.117.155\n","Connecting to zenodo.org (zenodo.org)|188.184.117.155|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1365409299 (1.3G) [application/octet-stream]\n","Saving to: ‘Cnn14_mAP=0.431.pth?download=1’\n","\n","Cnn14_mAP=0.431.pth 100%[===================>]   1.27G  11.3MB/s    in 94s     \n","\n","2020-04-07 08:16:04 (13.8 MB/s) - ‘Cnn14_mAP=0.431.pth?download=1’ saved [1365409299/1365409299]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f-EFpU0BtnTN","colab_type":"code","colab":{}},"source":["!mv 'Cnn14_mAP=0.431.pth?download=1' 'Cnn14_mAP=0.431.pth'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"adIYamAhq8oF","colab_type":"code","outputId":"4122bf16-f0ac-4f41-e727-327054cccb8f","executionInfo":{"status":"ok","timestamp":1586247588393,"user_tz":-540,"elapsed":14304,"user":{"displayName":"yoshihiro matsumoto","photoUrl":"","userId":"08203775409780696699"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["!MODEL_TYPE=\"Cnn14\"\n","!CHECKPOINT_PATH=\"Cnn14_mAP=0.431.pth\"\n","!CUDA_VISIBLE_DEVICES=0 python3 pytorch/inference.py audio_tagging --model_type=$MODEL_TYPE --checkpoint_path=$CHECKPOINT_PATH --audio_path=\"/content/audioset_tagging_cnn/examples/20200407124650.wav\" --cuda"],"execution_count":0,"outputs":[{"output_type":"stream","text":["GPU number: 1\n","Speech: 0.763\n","Speech synthesizer: 0.149\n","Male speech, man speaking: 0.130\n","Vehicle: 0.099\n","Radio: 0.095\n","Narration, monologue: 0.087\n","White noise: 0.085\n","Inside, small room: 0.043\n","Telephone: 0.030\n","Air conditioning: 0.025\n","embedding: (2048,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jiY3ZexBrpQ4","colab_type":"code","outputId":"d83b0cfe-3e53-4d9f-b7af-98ffd56a25c7","executionInfo":{"status":"ok","timestamp":1586247386771,"user_tz":-540,"elapsed":1901,"user":{"displayName":"yoshihiro matsumoto","photoUrl":"","userId":"08203775409780696699"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" \u001b[0m\u001b[01;34mappendixes\u001b[0m/                       \u001b[01;34mkeras\u001b[0m/        R9_ZSCveAHg_7s.wav   \u001b[01;34mutils\u001b[0m/\n","'Cnn10_mAP=0.380.pth?download=1'   LICENSE.MIT   README.md\n","'Cnn14_mAP=0.431.pth?download=1'   \u001b[01;34mmetadata\u001b[0m/     requirements.txt\n"," \u001b[01;34mexamples\u001b[0m/                         \u001b[01;34mpytorch\u001b[0m/      runme.sh\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qkPnb3ratj38","colab_type":"code","outputId":"61cc5b84-d39d-4670-d6e4-ca6a256660a2","executionInfo":{"status":"ok","timestamp":1586249648399,"user_tz":-540,"elapsed":8769,"user":{"displayName":"yoshihiro matsumoto","photoUrl":"","userId":"08203775409780696699"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!MODEL_TYPE=\"Transfer_Cnn14\"\n","!CHECKPOINT_PATH=\"Cnn14_mAP=0.431.pth\"\n","!CUDA_VISIBLE_DEVICES=1 python3 pytorch/finetune_template.py train --window_size=1024 --hop_size=320 --mel_bins=64 --fmin=50 --fmax=14000 --model_type=$MODEL_TYPE --pretrained_checkpoint_path=$CHECKPOINT_PATH --cuda"],"execution_count":0,"outputs":[{"output_type":"stream","text":["GPU number: 0\n","Load pretrained model successfully!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"le6I3XX9z5hE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ltANCM2y2hk5","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dF-tplGW2hQW","colab_type":"code","colab":{}},"source":["# finetune_template.py\n","import os\n","import sys\n","sys.path.insert(1, os.path.join(sys.path[0], '../utils'))\n","import numpy as np\n","import argparse\n","import h5py\n","import math\n","import time\n","import logging\n","import matplotlib.pyplot as plt\n","\n","import torch\n","torch.backends.cudnn.benchmark=True\n","torch.manual_seed(0)\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data\n"," \n","from utilities import get_filename\n","from models import *\n","import config\n","\n","\n","class Transfer_Cnn14(nn.Module):\n","    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n","        fmax, classes_num, freeze_base):\n","        \"\"\"Classifier for a new task using pretrained Cnn14 as a sub module.\n","        \"\"\"\n","        super(Transfer_Cnn14, self).__init__()\n","        audioset_classes_num = 527\n","        \n","        self.base = Cnn14(sample_rate, window_size, hop_size, mel_bins, fmin, \n","            fmax, audioset_classes_num)\n","\n","        if freeze_base:\n","            for param in self.base.parameters():\n","                param.requires_grad = False\n","        \n","        self.fc_transfer = nn.Linear(2048, classes_num, bias=True)\n","\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        init_layer(self.fc_transfer)\n","\n","    def load_from_pretrain(self, pretrained_checkpoint_path):\n","        checkpoint = torch.load(pretrained_checkpoint_path, map_location='cpu' )\n","        self.base.load_state_dict(checkpoint['model'])\n","        \n","\n","    def forward(self, input, mixup_lambda=None):\n","        '''\n","        Input: (batch_size, data_length)'''\n","        output_dict = self.base(input, mixup_lambda)\n","        embedding = output_dict['embedding']\n","\n","        clipwise_output =  torch.log_softmax(self.fc_transfer(embedding), dim=-1)\n","        output_dict['clipwise_output'] = clipwise_output\n"," \n","        return output_dict\n","\n","\n","def train(args):\n","\n","    # Arugments & parameters\n","    window_size = args.window_size\n","    hop_size = args.hop_size\n","    mel_bins = args.mel_bins\n","    fmin = args.fmin\n","    fmax = args.fmax\n","    model_type = \"Transfer_Cnn14\"#args.model_type\n","    pretrained_checkpoint_path = \"Cnn14_mAP=0.431.pth\"#args.pretrained_checkpoint_path\n","    freeze_base = args.freeze_base\n","    device = 'cuda' if (args.cuda and torch.cuda.is_available()) else 'cpu'\n","\n","    sample_rate = config.sample_rate\n","    classes_num = config.classes_num\n","    pretrain = True if pretrained_checkpoint_path else False\n","    \n","    # Model\n","    Model = eval(model_type)\n","    model = Model(sample_rate, window_size, hop_size, mel_bins, fmin, fmax, \n","        classes_num, freeze_base)\n","\n","    # Load pretrained model\n","    if pretrain:\n","        logging.info('Load pretrained model from {}'.format(pretrained_checkpoint_path))\n","        model.load_from_pretrain(pretrained_checkpoint_path)\n","\n","    # Parallel\n","    print('GPU number: {}'.format(torch.cuda.device_count()))\n","    model = torch.nn.DataParallel(model)\n","\n","    if 'cuda' in device:\n","        model.to(device)\n","\n","    print('Load pretrained model successfully!')\n","\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser(description='Example of parser. ')\n","    subparsers = parser.add_subparsers(dest='mode')\n","\n","    # Train\n","    parser_train = subparsers.add_parser('train')\n","    parser_train.add_argument('--window_size', type=int, required=True)\n","    parser_train.add_argument('--hop_size', type=int, required=True)\n","    parser_train.add_argument('--mel_bins', type=int, required=True)\n","    parser_train.add_argument('--fmin', type=int, required=True)\n","    parser_train.add_argument('--fmax', type=int, required=True) \n","    parser_train.add_argument('--model_type', type=str, required=True)\n","    parser_train.add_argument('--pretrained_checkpoint_path', type=str)\n","    parser_train.add_argument('--freeze_base', action='store_true', default=False)\n","    parser_train.add_argument('--cuda', action='store_true', default=False)\n","\n","    # Parse arguments\n","    args = parser.parse_args()\n","    args.filename = get_filename(__file__)\n","\n","    if args.mode == 'train':\n","        train(args)\n","\n","    else:\n","        raise Exception('Error argument!')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bPqVsbqU2lJq","colab_type":"code","colab":{}},"source":["# inference.py\n","\n","import os\n","import sys\n","sys.path.insert(1, os.path.join(sys.path[0], '../utils'))\n","import numpy as np\n","import argparse\n","import librosa\n","import matplotlib.pyplot as plt\n","import torch\n","\n","from utilities import create_folder, get_filename\n","from models import *\n","from pytorch_utils import move_data_to_device\n","import config\n","\n","\n","def audio_tagging(args):\n","    \"\"\"Inference audio tagging result of an audio clip.\n","    \"\"\"\n","\n","    # Arugments & parameters\n","    window_size = args.window_size\n","    hop_size = args.hop_size\n","    mel_bins = args.mel_bins\n","    fmin = args.fmin\n","    fmax = args.fmax\n","    model_type = \"Cnn14\"#args.model_type\n","    checkpoint_path = \"Cnn14_mAP=0.431.pth\"#args.checkpoint_path\n","    audio_path = args.audio_path\n","    device = torch.device('cuda') if args.cuda and torch.cuda.is_available() else torch.device('cpu')\n","\n","    sample_rate = config.sample_rate\n","    classes_num = config.classes_num\n","    labels = config.labels\n","\n","    # Model\n","    Model = eval(model_type)\n","    model = Model(sample_rate=sample_rate, window_size=window_size, \n","        hop_size=hop_size, mel_bins=mel_bins, fmin=fmin, fmax=fmax, \n","        classes_num=classes_num)\n","    \n","    checkpoint = torch.load(checkpoint_path, map_location=device)\n","    model.load_state_dict(checkpoint['model'])\n","\n","    # Parallel\n","    print('GPU number: {}'.format(torch.cuda.device_count()))\n","    model = torch.nn.DataParallel(model)\n","\n","    if 'cuda' in str(device):\n","        model.to(device)\n","    \n","    # Load audio\n","    (waveform, _) = librosa.core.load(audio_path, sr=sample_rate, mono=True)\n","\n","    waveform = waveform[None, :]    # (1, audio_length)\n","    waveform = move_data_to_device(waveform, device)\n","\n","    # Forward\n","    model.eval()\n","    batch_output_dict = model(waveform, None)\n","\n","    clipwise_output = batch_output_dict['clipwise_output'].data.cpu().numpy()[0]\n","    \"\"\"(classes_num,)\"\"\"\n","\n","    sorted_indexes = np.argsort(clipwise_output)[::-1]\n","\n","    # Print audio tagging top probabilities\n","    for k in range(10):\n","        print('{}: {:.3f}'.format(np.array(labels)[sorted_indexes[k]], \n","            clipwise_output[sorted_indexes[k]]))\n","\n","    if 'embedding' in batch_output_dict.keys():\n","        embedding = batch_output_dict['embedding'].data.cpu().numpy()[0]\n","        print('embedding: {}'.format(embedding.shape))\n","\n","    return clipwise_output, labels\n","\n","\n","def sound_event_detection(args):\n","    \"\"\"Inference sound event detection result of an audio clip.\n","    \"\"\"\n","\n","    # Arugments & parameters\n","    window_size = args.window_size\n","    hop_size = args.hop_size\n","    mel_bins = args.mel_bins\n","    fmin = args.fmin\n","    fmax = args.fmax\n","    model_type = args.model_type\n","    checkpoint_path = args.checkpoint_path\n","    audio_path = args.audio_path\n","    device = torch.device('cuda') if args.cuda and torch.cuda.is_available() else torch.device('cpu')\n","\n","    sample_rate = config.sample_rate\n","    classes_num = config.classes_num\n","    labels = config.labels\n","    frames_per_second = sample_rate // hop_size\n","\n","    # Paths\n","    fig_path = os.path.join('results', '{}.png'.format(get_filename(audio_path)))\n","    create_folder(os.path.dirname(fig_path))\n","\n","    # Model\n","    Model = eval(model_type)\n","    model = Model(sample_rate=sample_rate, window_size=window_size, \n","        hop_size=hop_size, mel_bins=mel_bins, fmin=fmin, fmax=fmax, \n","        classes_num=classes_num)\n","    \n","    checkpoint = torch.load(checkpoint_path, map_location=device)\n","    model.load_state_dict(checkpoint['model'])\n","\n","    # Parallel\n","    print('GPU number: {}'.format(torch.cuda.device_count()))\n","    model = torch.nn.DataParallel(model)\n","\n","    if 'cuda' in str(device):\n","        model.to(device)\n","    \n","    # Load audio\n","    (waveform, _) = librosa.core.load(audio_path, sr=sample_rate, mono=True)\n","\n","    waveform = waveform[None, :]    # (1, audio_length)\n","    waveform = move_data_to_device(waveform, device)\n","\n","    # Forward\n","    model.eval()\n","    batch_output_dict = model(waveform, None)\n","\n","    framewise_output = batch_output_dict['framewise_output'].data.cpu().numpy()[0]\n","    \"\"\"(time_steps, classes_num)\"\"\"\n","\n","    print('Sound event detection result (time_steps x classes_num): {}'.format(framewise_output.shape))\n","\n","    sorted_indexes = np.argsort(np.max(framewise_output, axis=0))[::-1]\n","\n","    top_k = 10  # Show top results\n","    top_result_mat = framewise_output[:, sorted_indexes[0 : top_k]]    \n","    \"\"\"(time_steps, top_k)\"\"\"\n","\n","    # Plot result    \n","    stft = librosa.core.stft(y=waveform[0].data.cpu().numpy(), n_fft=window_size, \n","        hop_length=hop_size, window='hann', center=True)\n","    frames_num = stft.shape[-1]\n","\n","    fig, axs = plt.subplots(2, 1, sharex=True, figsize=(10, 4))\n","    axs[0].matshow(np.log(np.abs(stft)), origin='lower', aspect='auto', cmap='jet')\n","    axs[0].set_ylabel('Frequency bins')\n","    axs[0].set_title('Log spectrogram')\n","    axs[1].matshow(top_result_mat.T, origin='upper', aspect='auto', cmap='jet', vmin=0, vmax=1)\n","    axs[1].xaxis.set_ticks(np.arange(0, frames_num, frames_per_second))\n","    axs[1].xaxis.set_ticklabels(np.arange(0, frames_num / frames_per_second))\n","    axs[1].yaxis.set_ticks(np.arange(0, top_k))\n","    axs[1].yaxis.set_ticklabels(np.array(labels)[sorted_indexes[0 : top_k]])\n","    axs[1].yaxis.grid(color='k', linestyle='solid', linewidth=0.3, alpha=0.3)\n","    axs[1].set_xlabel('Seconds')\n","    axs[1].xaxis.set_ticks_position('bottom')\n","\n","    plt.tight_layout()\n","    plt.savefig(fig_path)\n","    print('Save sound event detection visualization to {}'.format(fig_path))\n","\n","    return framewise_output, labels\n","\n","\n","if __name__ == '__main__':\n","\n","    parser = argparse.ArgumentParser(description='Example of parser. ')\n","    subparsers = parser.add_subparsers(dest='mode')\n","\n","    parser_at = subparsers.add_parser('audio_tagging')\n","    parser_at.add_argument('--window_size', type=int, default=1024)\n","    parser_at.add_argument('--hop_size', type=int, default=320)\n","    parser_at.add_argument('--mel_bins', type=int, default=64)\n","    parser_at.add_argument('--fmin', type=int, default=50)\n","    parser_at.add_argument('--fmax', type=int, default=14000) \n","    parser_at.add_argument('--model_type', type=str, required=True)\n","    parser_at.add_argument('--checkpoint_path', type=str, required=True)\n","    parser_at.add_argument('--audio_path', type=str, required=True)\n","    parser_at.add_argument('--cuda', action='store_true', default=False)\n","\n","    parser_sed = subparsers.add_parser('sound_event_detection')\n","    parser_sed.add_argument('--window_size', type=int, default=1024)\n","    parser_sed.add_argument('--hop_size', type=int, default=320)\n","    parser_sed.add_argument('--mel_bins', type=int, default=64)\n","    parser_sed.add_argument('--fmin', type=int, default=50)\n","    parser_sed.add_argument('--fmax', type=int, default=14000) \n","    parser_sed.add_argument('--model_type', type=str, required=True)\n","    parser_sed.add_argument('--checkpoint_path', type=str, required=True)\n","    parser_sed.add_argument('--audio_path', type=str, required=True)\n","    parser_sed.add_argument('--cuda', action='store_true', default=False)\n","    \n","    args = parser.parse_args()\n","\n","    if args.mode == 'audio_tagging':\n","        audio_tagging(args)\n","\n","    elif args.mode == 'sound_event_detection':\n","        sound_event_detection(args)\n","\n","    else:\n","        raise Exception('Error argument!')\n"],"execution_count":0,"outputs":[]}]}